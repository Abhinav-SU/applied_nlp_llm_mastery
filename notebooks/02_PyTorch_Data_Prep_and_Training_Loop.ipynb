{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "670218d0-f436-4174-8965-b8f3ddbb02eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in dataset: 1000\n",
      "First sample features: tensor([-0.2876, -0.9066,  0.8859,  0.2278,  1.0939]), label: tensor([0.])\n",
      "\n",
      "Iterating through DataLoader with batch_size=32:\n",
      "Batch 1: Features shape torch.Size([32, 5]), Labels shape torch.Size([32, 1])\n",
      "Batch 2: Features shape torch.Size([32, 5]), Labels shape torch.Size([32, 1])\n",
      "Batch 3: Features shape torch.Size([32, 5]), Labels shape torch.Size([32, 1])\n",
      "\n",
      "DataLoader will automatically handle batching, shuffling, and memory management!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Create a Custom Dataset\n",
    "class CustomDummyDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, num_features=5):\n",
    "        self.data = torch.randn(num_samples, num_features)  # Dummy input features\n",
    "        self.labels = torch.randint(0, 2, (num_samples, 1)).float() # Dummy binary labels (0 or 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) # Total number of samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return one sample (features and its corresponding label)\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 2. Instantiate the Dataset\n",
    "my_dataset = CustomDummyDataset(num_samples=1000, num_features=5)\n",
    "print(f\"Total samples in dataset: {len(my_dataset)}\")\n",
    "\n",
    "# Get a single sample directly (just for demonstration, DataLoader handles this usually)\n",
    "sample_features, sample_label = my_dataset[0]\n",
    "print(f\"First sample features: {sample_features}, label: {sample_label}\")\n",
    "\n",
    "# 3. Create a DataLoader\n",
    "# batch_size: how many samples per batch\n",
    "# shuffle: True means shuffle the data at the beginning of each epoch (training pass)\n",
    "# num_workers: how many separate processes to use for data loading (0 means main process)\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# 4. Iterate through the DataLoader to get batches\n",
    "print(f\"\\nIterating through DataLoader with batch_size={batch_size}:\")\n",
    "for i, (features_batch, labels_batch) in enumerate(train_dataloader):\n",
    "    print(f\"Batch {i+1}: Features shape {features_batch.shape}, Labels shape {labels_batch.shape}\")\n",
    "    if i == 2: # Just print the first 3 batches to avoid too much output\n",
    "        break\n",
    "\n",
    "print(\"\\nDataLoader will automatically handle batching, shuffling, and memory management!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce017a29-52f5-4257-bb87-6f827fa02410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Starting Training...\n",
      "Epoch [1/50], Average Loss: 2.8987\n",
      "Epoch [10/50], Average Loss: 0.0250\n",
      "Epoch [20/50], Average Loss: 0.0150\n",
      "Epoch [30/50], Average Loss: 0.0129\n",
      "Epoch [40/50], Average Loss: 0.0123\n",
      "Epoch [50/50], Average Loss: 0.0122\n",
      "\n",
      "Training Finished!\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have MySimpleRegressor, CustomDummyDataset, and DataLoader defined from previous steps\n",
    "# (You'll put this in your 02_PyTorch_Data_Prep_and_Training_Loop.ipynb)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Re-define our Dataset and Model (for completeness in this single code block) ---\n",
    "\n",
    "class CustomDummyDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, num_features=5):\n",
    "        # Let's adjust this for a simple regression problem: features and a continuous target\n",
    "        self.data = torch.randn(num_samples, num_features)\n",
    "        # Create labels as a simple linear relationship + some noise\n",
    "        self.labels = (self.data[:, 0] * 2 + self.data[:, 1] * 0.5 + 1.0 + torch.randn(num_samples) * 0.1).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "class MySimpleRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MySimpleRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# --- Configuration for our training ---\n",
    "input_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 50 # How many times we'll go through the entire dataset\n",
    "batch_size = 32\n",
    "num_samples = 1000\n",
    "\n",
    "# 1. Instantiate the Dataset and DataLoader\n",
    "dataset = CustomDummyDataset(num_samples=num_samples, num_features=input_dim)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 2. Instantiate the Model\n",
    "model = MySimpleRegressor(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# 3. Define Loss Function (for regression, we use MSE Loss)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 4. Define Optimizer (Adam is a good general-purpose choice)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check if GPU is available and move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- The Training Loop ---\n",
    "print(\"\\nStarting Training...\")\n",
    "for epoch in range(epochs):\n",
    "    # 'train_loss' will accumulate loss for this epoch\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Iterate over batches of data from the DataLoader\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        # Move inputs and targets to the same device as the model (GPU if available)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # 1. Forward Pass: Make a prediction\n",
    "        predictions = model(inputs)\n",
    "\n",
    "        # 2. Calculate Loss\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        train_loss += loss.item() # Add loss to accumulator for logging\n",
    "\n",
    "        # 3. Zero the Gradients: Clear old gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Backward Pass: Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer Step: Update model weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_train_loss = train_loss / len(dataloader)\n",
    "\n",
    "    # Print progress (optional, but good for tracking)\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0: # Print every 10 epochs and first epoch\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc31be-c875-4b6c-99c9-1d2f8501e1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
