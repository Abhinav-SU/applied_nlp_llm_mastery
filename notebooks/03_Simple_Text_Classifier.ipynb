{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e6208f-4c81-4173-8a70-6c2d98793f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'love', 'this', 'movie'], ['this', 'film', 'is', 'great'], ['the', 'acting', 'was', 'terrible'], ['i', 'hate', 'this', 'film'], ['great', 'movie'], ['it', 'was', 'good']]\n",
      "\n",
      "Numericalized Reviews as PyTorch Tensor:\n",
      "tensor([[ 7, 10, 14, 11,  0],\n",
      "        [14,  3,  8,  5,  0],\n",
      "        [13,  2, 15, 12,  0],\n",
      "        [ 7,  6, 14,  3,  0],\n",
      "        [ 5, 11,  0,  0,  0],\n",
      "        [ 9, 15,  4,  0,  0]])\n",
      "Tensor shape: torch.Size([6, 5])\n",
      "\n",
      "Labels Tensor:\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Labels shape: torch.Size([6, 1])\n",
      "\n",
      "Starting Training for Sentiment Classifier...\n",
      "Epoch [1/100], Loss: 0.7170, Accuracy: 1.0000\n",
      "Epoch [10/100], Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch [20/100], Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch [30/100], Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch [40/100], Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch [50/100], Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch [60/100], Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch [70/100], Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch [80/100], Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch [90/100], Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch [100/100], Loss: 0.0000, Accuracy: 1.0000\n",
      "\n",
      "Training Finished!\n",
      "\n",
      "--- Testing Inference on New Sentences ---\n",
      "'This is a great film! I really enjoyed it.' -> Predicted: Positive (Probability: 1.0000)\n",
      "'This movie was absolutely awful and boring.' -> Predicted: Positive (Probability: 0.9908)\n",
      "'It was just okay.' -> Predicted: Positive (Probability: 1.0000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "movie_reviews=[\"I love this movie!\",\"This film is great.\",\"The acting was terrible.\",\"I hate this film.\",\"Great movie!\",\"It was good.\"]\n",
    "moview_labels=[1,1,0,0,1,1]\n",
    "\n",
    "tokenized_reviews= [review.lower().replace(\"!\",\"\").replace(\".\",\"\").split() for review in movie_reviews]\n",
    "print(tokenized_reviews)\n",
    "\n",
    "all_words = [word for words in tokenized_reviews for word in words]\n",
    "unique_words = set(all_words)\n",
    "\n",
    "unique_words.add(\"<UNK>\")\n",
    "unique_words.add(\"<PAD>\")\n",
    "\n",
    "vocabulary=[]\n",
    "vocabulary=sorted(unique_words)\n",
    "\n",
    "word_to_idx={word : indx for indx,word in enumerate(vocabulary)}\n",
    "idx_to_word={indx:word for indx, word in enumerate(vocabulary)}\n",
    "\n",
    "max_sequence_length=5\n",
    "pad_idx = word_to_idx[\"<PAD>\"]\n",
    "numericalized_padded_reviews_list=[]\n",
    "\n",
    "\n",
    "\n",
    "for tokens in tokenized_reviews:\n",
    "    \n",
    "    current_numerical_sequence=[word_to_idx.get(word,word_to_idx[\"<UNK>\"]) for word in tokens]\n",
    "    \n",
    "    if len(current_numerical_sequence) < max_sequence_length:\n",
    "        \n",
    "        padding_needed = max_sequence_length - len(current_numerical_sequence)\n",
    "        final_sequence_for_nn = current_numerical_sequence + [pad_idx] * padding_needed\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        final_sequence_for_nn = current_numerical_sequence[:max_sequence_length]\n",
    "    \n",
    "    numericalized_padded_reviews_list.append(final_sequence_for_nn)\n",
    "\n",
    "    \n",
    "numericalized_reviews_tensor = torch.tensor(numericalized_padded_reviews_list, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(moview_labels, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "print(\"\\nNumericalized Reviews as PyTorch Tensor:\")\n",
    "print(numericalized_reviews_tensor)\n",
    "print(\"Tensor shape:\", numericalized_reviews_tensor.shape)\n",
    "\n",
    "print(\"\\nLabels Tensor:\")\n",
    "print(labels_tensor)\n",
    "print(\"Labels shape:\", labels_tensor.shape)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class TextSentimentDataset(Dataset):\n",
    "    def __init__(self,data_tensor,label_tensor):\n",
    "        super().__init__()\n",
    "        self.data = data_tensor\n",
    "        self.labels = label_tensor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self,x):\n",
    "        return self.data[x],self.labels[x]\n",
    "\n",
    "\n",
    "sentiment_dataset = TextSentimentDataset(numericalized_reviews_tensor,labels_tensor)\n",
    "\n",
    "sentiment_dataloader = DataLoader(sentiment_dataset,batch_size=32,shuffle=True)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleSentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size,max_sequence_length , embedding_dim, hidden_size, num_classes=1):\n",
    "        super().__init__()\n",
    "        # We need to define our layers here\n",
    "        self.embedding_layer = nn.Embedding(vocab_size,embedding_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(max_sequence_length *embedding_dim,hidden_size)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size,num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.embedding_layer(x)\n",
    "        x=x.view(x.size(0), -1)\n",
    "        x=self.fc1(x)\n",
    "        x=F.relu(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "#Initializing all the obj variable to feed to sentimentCsass\n",
    "vocab_size = len(vocabulary) #or maybe batch_size?\n",
    "max_sequence_length = numericalized_reviews_tensor.size(1)\n",
    "embedding_dim = 10\n",
    "hidden_size = max_sequence_length*embedding_dim\n",
    "num_classes = 1\n",
    "\n",
    "#Object of sentimentClass\n",
    "model = SimpleSentimentClassifier(vocab_size,max_sequence_length,embedding_dim,hidden_size,num_classes)\n",
    "\n",
    "#Defining Loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Optimizers\n",
    "param = model.parameters()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam(param,lr=learning_rate)\n",
    "\n",
    "#Device Selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    \n",
    "model.to(device)\n",
    "\n",
    "\n",
    "epochs=100\n",
    "print(\"\\nStarting Training for Sentiment Classifier...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs , target in sentiment_dataloader:\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        loss = loss_fn(predictions,target)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(sentiment_dataloader)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Loop through the dataloader again (or use a separate validation set if available)\n",
    "        for inputs, target in sentiment_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            predictions = model(inputs) # Get raw logits\n",
    "            \n",
    "            # Apply sigmoid to convert logits to probabilities, then round to get binary class (0 or 1)\n",
    "            predicted_classes = torch.round(torch.sigmoid(predictions))\n",
    "            \n",
    "            # Count how many predictions match the true target\n",
    "            total_correct += (predicted_classes == target).sum().item()\n",
    "            total_samples += target.numel() # Count total number of elements (labels)\n",
    "            \n",
    "        accuracy = total_correct / total_samples\n",
    "\n",
    "    # Set the model back to training mode for the next epoch\n",
    "    model.train() # Important!\n",
    "\n",
    "    # Print progress (e.g., every 10 epochs or on the first epoch)\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0: # Print on epoch 1, 10, 20, ...\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nTraining Finished!\")\n",
    "\n",
    "# --- (Next, we'd add the final inference/test section) ---\n",
    "\n",
    "\n",
    "def predict_sentiment(text, model, word_to_idx, max_sequence_length,device):\n",
    "    model.eval()\n",
    "    # 1. Text Preprocessing (similar to what we did for training data)\n",
    "    # Convert to lowercase and remove common punctuation\n",
    "    clean_text = text.lower().replace(\"!\", \"\").replace(\".\", \"\").replace(\",\", \"\")\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    tokens = clean_text.split()\n",
    "    \n",
    "    # Numericalize: Convert words to indices using our word_to_idx map\n",
    "    # Handle unknown words by mapping them to the <UNK> token's index\n",
    "    numericalized_tokens = [word_to_idx.get(token, word_to_idx[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "    # Pad or Truncate the sequence to max_sequence_length\n",
    "    if len(numericalized_tokens) < max_sequence_length:\n",
    "        padding_needed = max_sequence_length - len(numericalized_tokens)\n",
    "        padded_tokens = numericalized_tokens + [word_to_idx[\"<PAD>\"]] * padding_needed\n",
    "    else:\n",
    "        padded_tokens = numericalized_tokens[:max_sequence_length]\n",
    "\n",
    "    # Convert the processed list to a PyTorch tensor\n",
    "    # .unsqueeze(0) adds a batch dimension (since our model expects batches, even for a single sample)\n",
    "    input_tensor = torch.tensor(padded_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # 2. Model Inference\n",
    "    # Disable gradient calculations (important for inference/evaluation)\n",
    "    with torch.no_grad():\n",
    "        # Get raw logits from the model\n",
    "        output_logits = model(input_tensor)\n",
    "        \n",
    "        # Apply sigmoid to convert logits to a probability between 0 and 1\n",
    "        prediction_prob = torch.sigmoid(output_logits).item() # .item() gets the Python number from a 1-element tensor\n",
    "\n",
    "    # 3. Determine the final class prediction\n",
    "    predicted_class = \"Positive\" if prediction_prob >= 0.5 else \"Negative\"\n",
    "    \n",
    "    return predicted_class, prediction_prob\n",
    "\n",
    "# --- Testing with new sentences (Add this after the function definition) ---\n",
    "print(\"\\n--- Testing Inference on New Sentences ---\")\n",
    "\n",
    "test_sentence1 = \"This is a great film! I really enjoyed it.\"\n",
    "test_sentence2 = \"This movie was absolutely awful and boring.\"\n",
    "test_sentence3 = \"It was just okay.\" # A tricky one, might be difficult for a tiny model trained on tiny data\n",
    "\n",
    "# Call the function to predict sentiment for each sentence\n",
    "sentiment1, prob1 = predict_sentiment(test_sentence1, model, word_to_idx, max_sequence_length, device)\n",
    "print(f\"'{test_sentence1}' -> Predicted: {sentiment1} (Probability: {prob1:.4f})\")\n",
    "\n",
    "sentiment2, prob2 = predict_sentiment(test_sentence2, model, word_to_idx, max_sequence_length, device)\n",
    "print(f\"'{test_sentence2}' -> Predicted: {sentiment2} (Probability: {prob2:.4f})\")\n",
    "\n",
    "sentiment3, prob3 = predict_sentiment(test_sentence3, model, word_to_idx, max_sequence_length, device)\n",
    "print(f\"'{test_sentence3}' -> Predicted: {sentiment3} (Probability: {prob3:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9856b1-6895-4002-bafb-dda5a9abbec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for training: cuda\n",
      "\n",
      "Starting Training for Sentiment Classifier...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([2, 1])) must be the same as input size (torch.Size([2, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 240\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# This ensures main() runs when the script is executed\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 194\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Forward pass, loss calculation, backpropagation, and optimization step\u001b[39;00m\n\u001b[32m    193\u001b[39m predictions = model(inputs)\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m train_loss += loss.item()\n\u001b[32m    197\u001b[39m optimizer.zero_grad() \u001b[38;5;66;03m# Zero gradients\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Machine Learning\\applied_nlp_llm_mastery\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Machine Learning\\applied_nlp_llm_mastery\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Machine Learning\\applied_nlp_llm_mastery\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:821\u001b[39m, in \u001b[36mBCEWithLogitsLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Machine Learning\\applied_nlp_llm_mastery\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3639\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n\u001b[32m   3636\u001b[39m     reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m   3638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28minput\u001b[39m.size()):\n\u001b[32m-> \u001b[39m\u001b[32m3639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3640\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3641\u001b[39m     )\n\u001b[32m   3643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.binary_cross_entropy_with_logits(\n\u001b[32m   3644\u001b[39m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[32m   3645\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Target size (torch.Size([2, 1])) must be the same as input size (torch.Size([2, 2]))"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time # To track training time\n",
    "import numpy as np # Often used for numerical operations\n",
    "\n",
    "# --- 1. Data Definition ---\n",
    "movie_reviews = [\n",
    "    \"I love this movie!\",\n",
    "    \"This film is great.\",\n",
    "    \"The acting was terrible.\",\n",
    "    \"I hate this film.\",\n",
    "    \"Great movie!\",\n",
    "    \"It was good.\",\n",
    "]\n",
    "movie_labels = [1, 1, 0, 0, 1, 1] # 1 for positive, 0 for negative\n",
    "\n",
    "# --- 2. Text Preprocessing ---\n",
    "def preprocess_text(reviews, max_seq_len, word_to_idx, pad_idx, unk_idx):\n",
    "    \"\"\"\n",
    "    Tokenizes, numericalizes, and pads/truncates a list of raw text reviews.\n",
    "    \"\"\"\n",
    "    tokenized_reviews = [\n",
    "        review.lower().replace(\"!\", \"\").replace(\".\", \"\").replace(\",\", \"\").split()\n",
    "        for review in reviews\n",
    "    ]\n",
    "\n",
    "    numericalized_padded_reviews_list = []\n",
    "    for tokens in tokenized_reviews:\n",
    "        # Numericalize: Convert words to indices, handling unknown words\n",
    "        current_numerical_sequence = [\n",
    "            word_to_idx.get(word, unk_idx) for word in tokens\n",
    "        ]\n",
    "\n",
    "        # Pad or Truncate: Make all sequences max_sequence_length\n",
    "        if len(current_numerical_sequence) < max_seq_len:\n",
    "            padding_needed = max_seq_len - len(current_numerical_sequence)\n",
    "            final_sequence_for_nn = current_numerical_sequence + [pad_idx] * padding_needed\n",
    "        else:\n",
    "            final_sequence_for_nn = current_numerical_sequence[:max_seq_len]\n",
    "\n",
    "        numericalized_padded_reviews_list.append(final_sequence_for_nn)\n",
    "\n",
    "    return numericalized_padded_reviews_list\n",
    "\n",
    "# --- Build Vocabulary (needs to be done once from all data) ---\n",
    "all_words = [word for review_tokens in (\n",
    "    r.lower().replace(\"!\", \"\").replace(\".\", \"\").replace(\",\", \"\").split() for r in movie_reviews\n",
    ") for word in review_tokens]\n",
    "\n",
    "unique_words = set(all_words)\n",
    "unique_words.add(\"<UNK>\") # For unknown words\n",
    "unique_words.add(\"<PAD>\") # For padding shorter sequences\n",
    "\n",
    "vocabulary = sorted(list(unique_words)) # Ensure consistent order\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Define necessary preprocessing constants\n",
    "MAX_SEQUENCE_LENGTH = 5 # Max length of a review sequence\n",
    "PAD_IDX = word_to_idx[\"<PAD>\"]\n",
    "UNK_IDX = word_to_idx[\"<UNK>\"]\n",
    "VOCAB_SIZE = len(vocabulary)\n",
    "\n",
    "# Preprocess the data using the defined function and constants\n",
    "numericalized_reviews_list = preprocess_text(\n",
    "    movie_reviews, MAX_SEQUENCE_LENGTH, word_to_idx, PAD_IDX, UNK_IDX\n",
    ")\n",
    "\n",
    "# Convert preprocessed data and labels to PyTorch tensors\n",
    "numericalized_reviews_tensor = torch.tensor(\n",
    "    numericalized_reviews_list, dtype=torch.long\n",
    ")\n",
    "labels_tensor = torch.tensor(movie_labels, dtype=torch.float).unsqueeze(1) # Unsqueeze for BCEWithLogitsLoss\n",
    "\n",
    "# --- 3. Custom Dataset and DataLoader ---\n",
    "class TextSentimentDataset(Dataset):\n",
    "    def __init__(self, data_tensor, label_tensor):\n",
    "        self.data = data_tensor\n",
    "        self.labels = label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# --- 4. Define the Model (SimpleSentimentClassifier) ---\n",
    "class SimpleSentimentClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, max_sequence_length, embedding_dim, hidden_size, num_classes=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Embedding layer: converts word indices to dense vectors\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # First fully connected layer (input to hidden)\n",
    "        # Input size is flattened sequence length * embedding dimension\n",
    "        self.fc1 = nn.Linear(max_sequence_length * embedding_dim, hidden_size)\n",
    "\n",
    "        # Second fully connected layer (hidden to output)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (batch_size, max_sequence_length) with word indices\n",
    "        \n",
    "        # 1. Pass through embedding layer\n",
    "        # Output shape: (batch_size, max_sequence_length, embedding_dim)\n",
    "        x = self.embedding_layer(x)\n",
    "        \n",
    "        # 2. Flatten the embeddings for the feedforward layer\n",
    "        # Output shape: (batch_size, max_sequence_length * embedding_dim)\n",
    "        x = x.view(x.size(0), -1) # -1 infers the dimension based on total elements\n",
    "\n",
    "        # 3. Pass through first linear layer with ReLU activation\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # 4. Pass through final linear layer (outputs raw logits)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# --- 5. Inference Function ---\n",
    "def predict_sentiment(text, model, word_to_idx, max_seq_len, device, unk_idx, pad_idx):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a single raw text string using the trained model.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    \n",
    "    # Preprocess the input text (same logic as training data)\n",
    "    clean_text = text.lower().replace(\"!\", \"\").replace(\".\", \"\").replace(\",\", \"\")\n",
    "    tokens = clean_text.split()\n",
    "    numericalized_tokens = [word_to_idx.get(token, unk_idx) for token in tokens]\n",
    "\n",
    "    if len(numericalized_tokens) < max_seq_len:\n",
    "        padded_tokens = numericalized_tokens + [pad_idx] * (max_seq_len - len(numericalized_tokens))\n",
    "    else:\n",
    "        padded_tokens = numericalized_tokens[:max_seq_len]\n",
    "\n",
    "    # Convert to PyTorch tensor and add batch dimension, move to device\n",
    "    input_tensor = torch.tensor(padded_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Perform inference (no gradient calculation)\n",
    "    with torch.no_grad():\n",
    "        output_logits = model(input_tensor)\n",
    "        prediction_prob = torch.sigmoid(output_logits).item() # Convert logits to probability\n",
    "\n",
    "    predicted_class = \"Positive\" if prediction_prob >= 0.5 else \"Negative\"\n",
    "    \n",
    "    return predicted_class, prediction_prob\n",
    "\n",
    "\n",
    "# --- Main Training and Testing Execution Block ---\n",
    "def main():\n",
    "    # --- Training Configuration ---\n",
    "    EMBEDDING_DIM = 10\n",
    "    HIDDEN_SIZE = MAX_SEQUENCE_LENGTH * EMBEDDING_DIM # Can also be an independent choice\n",
    "    NUM_CLASSES = 2\n",
    "    LEARNING_RATE = 0.005\n",
    "    EPOCHS = 100\n",
    "    BATCH_SIZE = 2 # Small for tiny dataset\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    sentiment_dataset = TextSentimentDataset(numericalized_reviews_tensor, labels_tensor)\n",
    "    sentiment_dataloader = DataLoader(sentiment_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # --- Model Instantiation ---\n",
    "    model = SimpleSentimentClassifier(\n",
    "        VOCAB_SIZE, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, HIDDEN_SIZE, NUM_CLASSES\n",
    "    )\n",
    "\n",
    "    # --- Loss Function and Optimizer Definition ---\n",
    "    loss_fn = nn.BCEWithLogitsLoss() # Combines Sigmoid and Binary Cross-Entropy for stability\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # --- Device Setup (CPU vs GPU) ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Using device for training: {device}\")\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    print(\"\\nStarting Training for Sentiment Classifier...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train() # Set model to training mode\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in sentiment_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # Move data to device\n",
    "\n",
    "            # Forward pass, loss calculation, backpropagation, and optimization step\n",
    "            predictions = model(inputs)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad() # Zero gradients\n",
    "            loss.backward()       # Backpropagation\n",
    "            optimizer.step()      # Update weights\n",
    "\n",
    "        avg_train_loss = train_loss / len(sentiment_dataloader)\n",
    "\n",
    "        # --- Evaluation on training data (simple check, not for true performance) ---\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        with torch.no_grad(): # Disable gradient calculations\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            for inputs, targets in sentiment_dataloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                predictions = model(inputs)\n",
    "                predicted_classes = torch.round(torch.sigmoid(predictions)) # Probabilities to 0/1 classes\n",
    "                total_correct += (predicted_classes == targets).sum().item()\n",
    "                total_samples += targets.numel()\n",
    "            accuracy = total_correct / total_samples\n",
    "\n",
    "        # Print progress report\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 1:\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTraining Finished! Total time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # --- Testing Inference on New Sentences ---\n",
    "    print(\"\\n--- Testing Inference on New Sentences ---\")\n",
    "    test_sentence1 = \"This is a great film! I really enjoyed it.\"\n",
    "    test_sentence2 = \"This movie was absolutely awful and boring.\"\n",
    "    test_sentence3 = \"It was just okay.\" # A tricky one, might be difficult for a tiny model trained on tiny data\n",
    "\n",
    "    sentiment1, prob1 = predict_sentiment(test_sentence1, model, word_to_idx, MAX_SEQUENCE_LENGTH, device, UNK_IDX, PAD_IDX)\n",
    "    print(f\"'{test_sentence1}' -> Predicted: {sentiment1} (Probability: {prob1:.4f})\")\n",
    "\n",
    "    sentiment2, prob2 = predict_sentiment(test_sentence2, model, word_to_idx, MAX_SEQUENCE_LENGTH, device, UNK_IDX, PAD_IDX)\n",
    "    print(f\"'{test_sentence2}' -> Predicted: {sentiment2} (Probability: {prob2:.4f})\")\n",
    "\n",
    "    sentiment3, prob3 = predict_sentiment(test_sentence3, model, word_to_idx, MAX_SEQUENCE_LENGTH, device, UNK_IDX, PAD_IDX)\n",
    "    print(f\"'{test_sentence3}' -> Predicted: {sentiment3} (Probability: {prob3:.4f})\")\n",
    "\n",
    "# This ensures main() runs when the script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d934e1-68dd-497f-a2fa-6abeb4a1efed",
   "metadata": {},
   "source": [
    "# 💼 Interview Preparation Notes: Applied NLP & LLMs\n",
    "\n",
    "This section compiles key concepts, common questions, and strategic points for technical and behavioral interviews for Applied NLP Engineer roles, drawing directly from the learning roadmap and projects.\n",
    "\n",
    "---\n",
    "\n",
    "## Module 1: Deep Learning Fundamentals (Interview Focus)\n",
    "\n",
    "### 1. PyTorch Tensors\n",
    "\n",
    "* **Concise Definition:** PyTorch Tensors are multi-dimensional arrays, similar to NumPy arrays, but optimized for deep learning computations, especially on GPUs, and are the fundamental data structure for all operations in PyTorch.\n",
    "* **\"Why it Matters\" / Importance:**\n",
    "    * **GPU Acceleration:** Tensors can be easily moved to a GPU (`.to('cuda')`), which performs parallel computations vastly faster than a CPU, crucial for training large neural networks and LLMs.\n",
    "    * **Automatic Differentiation (Autograd):** Tensors are the foundation for Autograd, enabling automatic gradient computation, which is vital for model learning.\n",
    "    * **Universal Data Format:** All inputs, outputs, and model parameters (weights, biases) in PyTorch are Tensors.\n",
    "* **Key PyTorch API:**\n",
    "    * `torch.tensor([...])`: Create tensor from Python list.\n",
    "    * `torch.zeros()`, `torch.ones()`, `torch.rand()`, `torch.randn()`: Create tensors with specific initial values.\n",
    "    * `.shape`, `.size()`: Get the dimensions of a tensor.\n",
    "    * `.dtype`: Get the data type of a tensor.\n",
    "    * `.to('cuda')` / `.to('cpu')`: Move tensor between GPU and CPU.\n",
    "    * `.view()`, `.reshape()`: Reshape tensors (e.g., for \"flattening\").\n",
    "* **Common Pitfalls/Considerations:**\n",
    "    * **CPU vs. GPU:** Ensure both model and input data are on the same device.\n",
    "    * **Data Types:** Be mindful of `torch.float32` (common for calculations) vs. `torch.long` (common for indices/integers).\n",
    "    * **\"Why Tensors over Python lists or NumPy arrays?\":** For mathematical operations, NumPy is fine, but Tensors offer *built-in GPU acceleration* and *gradient tracking (Autograd)*, which NumPy does not. Python lists are too slow for numerical computations.\n",
    "\n",
    "### 2. Autograd and Calculus for ML\n",
    "\n",
    "* **Concise Definition:** Autograd is PyTorch's automatic differentiation engine that calculates gradients for all operations on tensors, enabling neural networks to learn by knowing how to adjust their parameters to minimize loss.\n",
    "* **\"Why it Matters\" / Importance:**\n",
    "    * **Enables Learning:** It's the core mechanism for backpropagation and gradient descent, allowing models to adjust weights iteratively.\n",
    "    * **Eliminates Manual Derivatives:** Automates the extremely complex and error-prone process of calculating derivatives for millions/billions of parameters.\n",
    "    * **Flexibility (Dynamic Graphs):** Autograd builds the computation graph on-the-fly, allowing for dynamic network structures (e.g., conditional logic, variable sequence lengths).\n",
    "* **Key PyTorch API:**\n",
    "    * `tensor.requires_grad = True`: Flag a tensor to track operations for gradient computation.\n",
    "    * `loss.backward()`: Triggers the backward pass, computing gradients of the loss with respect to all `requires_grad=True` tensors.\n",
    "    * `optimizer.zero_grad()`: Clears accumulated gradients before a new backward pass (crucial!).\n",
    "    * `with torch.no_grad():`: Context manager to temporarily disable gradient tracking (for inference/evaluation).\n",
    "* **Connecting Concepts (Calculus):**\n",
    "    * **Gradient:** A vector indicating the direction of steepest ascent of a function (we move in the opposite direction to minimize loss).\n",
    "    * **Partial Derivatives:** Used when a function has multiple inputs, measuring change with respect to one variable while holding others constant.\n",
    "    * **Chain Rule:** The fundamental calculus rule Autograd uses to efficiently compute gradients across multiple layers of a network (derivative of composite functions).\n",
    "* **Common Pitfalls/Debugging:**\n",
    "    * **Forgetting `optimizer.zero_grad()`:** Leads to gradients accumulating from previous steps, causing incorrect updates and convergence issues.\n",
    "    * **Accidental `requires_grad=True` in evaluation:** Wastes memory and computation. Use `torch.no_grad()`.\n",
    "    * **Detaching tensors:** Sometimes you need to `.detach()` a tensor to remove it from the computation graph if you don't need its gradients tracked.\n",
    "\n",
    "### 3. Neural Network Building Blocks (Neurons, Layers, Activation Functions)\n",
    "\n",
    "* **Concise Definition:** Neural networks are compositions of interconnected \"neurons\" organized into \"layers,\" which process data through weighted sums, biases, and non-linear activation functions to learn complex patterns.\n",
    "* **\"Why it Matters\" / Importance:** These are the fundamental components that allow deep learning models to approximate complex real-world functions and learn from data.\n",
    "* **Key Concepts:**\n",
    "    * **Neuron (Node):** Basic unit. Takes inputs, multiplies by weights, adds bias, applies activation.\n",
    "    * **Weights & Biases:** Learnable parameters adjusted during training to capture patterns and shift outputs.\n",
    "    * **Activation Functions:** Introduce non-linearity. Without them, a deep network would just be a linear model.\n",
    "        * **ReLU (`F.relu`):** `max(0, x)`. Efficient, common in hidden layers.\n",
    "        * **Sigmoid (`torch.sigmoid`):** Squashes to (0,1). Good for binary classification output probabilities.\n",
    "        * **Softmax (`F.softmax`):** Converts raw scores to a probability distribution (sum to 1). Good for multi-class classification output.\n",
    "    * **Layers:**\n",
    "        * **Input Layer:** Receives raw features.\n",
    "        * **Hidden Layer(s):** Perform intermediate computations, learn complex features.\n",
    "        * **Output Layer:** Produces final prediction, tailored to the task.\n",
    "    * **Forward Pass:** Data flowing sequentially through layers from input to output to generate predictions.\n",
    "* **`nn.Module` (Recap):** The base class for all network layers and models in PyTorch. It tracks parameters and handles GPU/CPU moves.\n",
    "* **`nn.Linear`:** A \"fully connected\" layer. Performs `Output = (Input * Weights) + Bias`. Used for general feature combination and transformation.\n",
    "* **`nn.Embedding` (Crucial for Text):**\n",
    "    * **What it is:** A specialized layer that takes **integer word indices** as input and outputs **dense, continuous vectors (embeddings)** for each index. These vectors are learnable.\n",
    "    * **\"Why more vectors (embeddings) instead of just indices?\":** Integer IDs are arbitrary and carry no semantic meaning or relationship (e.g., word \"apple\" is 1, \"banana\" is 2 - no inherent similarity). Embeddings learn to place semantically similar words close together in a multi-dimensional space, capturing meaning and context (e.g., vector for \"king\" is close to \"queen\").\n",
    "    * **\"Why `nn.Embedding` as the first layer?\":** Because the network needs a meaningful, numerical representation of words *before* it can perform any \"thinking\" (linear transformations) about sentiment or other text patterns. `nn.Embedding` is the \"translator\" from meaningless ID to meaningful profile. Higher `embedding_dim` allows for richer profiles.\n",
    "* **Common Pitfalls/Considerations:**\n",
    "    * **Non-linearity:** Forgetting activation functions means your network can only learn linear relationships.\n",
    "    * **Output Layer Activation:** Choosing the correct final activation based on task (Sigmoid for binary prob, Softmax for multi-class prob, none for regression).\n",
    "\n",
    "### 4. Loss Functions and Optimizers\n",
    "\n",
    "* **Concise Definition:** Loss functions quantify a model's prediction error, while optimizers use gradients to adjust model parameters to minimize that error.\n",
    "* **\"Why it Matters\" / Importance:** They form the core feedback loop that allows neural networks to \"learn\" and improve their performance over time.\n",
    "* **Key Concepts:**\n",
    "    * **Loss Function:** (Also Cost/Objective Function) A single number representing \"how wrong\" the model's prediction is. Goal: minimize.\n",
    "        * **Mean Squared Error (MSE) Loss (`nn.MSELoss`):** For **regression** (predicting continuous numbers). Penalizes larger errors more.\n",
    "        * **Binary Cross-Entropy (BCE) Loss:**\n",
    "            * **What it is:** Measures dissimilarity between predicted probabilities and true binary labels. Heavily penalizes confident wrong predictions.\n",
    "            * **`nn.BCEWithLogitsLoss`:** **Preferred for binary classification in PyTorch.** It takes raw model outputs (logits), *internally applies a Sigmoid activation*, and then calculates BCE loss. This combination is **numerically more stable** than applying Sigmoid and `nn.BCELoss` separately.\n",
    "    * **Optimizer:** Algorithm that updates model weights based on gradients.\n",
    "        * **Learning Rate (`lr`):** The step size for parameter updates. Critical hyperparameter. Too small = slow learning, stuck. Too large = overshoot, oscillate, diverge.\n",
    "        * **Stochastic Gradient Descent (SGD):** Basic optimizer, moves weights opposite to gradient.\n",
    "        * **Adam (`torch.optim.Adam`):** Popular, adaptive optimizer. Adjusts learning rate for each parameter based on historical gradients, often leading to faster and more stable convergence.\n",
    "* **Common Pitfalls/Debugging:**\n",
    "    * **Choosing Wrong Loss:** Using MSE for classification or BCE for multi-class.\n",
    "    * **Learning Rate Tuning:** Model not learning (too low) or diverging (too high).\n",
    "    * **Zeroing Gradients:** Forgetting `optimizer.zero_grad()` leads to accumulated gradients and incorrect updates.\n",
    "\n",
    "### 5. Data Preparation (`Dataset` & `DataLoader`)\n",
    "\n",
    "* **Concise Definition:** `Dataset` provides an interface to individual data samples, while `DataLoader` efficiently batches, shuffles, and loads these samples for training.\n",
    "* **\"Why it Matters\" / Importance:** Essential for handling large datasets efficiently and preparing them for neural network training.\n",
    "* **Key Concepts:**\n",
    "    * **`torch.utils.data.Dataset`:** \"The Librarian.\" Represents the collection of samples. You create a custom class implementing:\n",
    "        * `__len__(self)`: Returns total number of samples.\n",
    "        * `__getitem__(self, idx)`: Returns a single `(input_data, label)` pair for a given index.\n",
    "    * **`torch.utils.data.DataLoader`:** \"The Delivery Service.\" Wraps a `Dataset` and provides:\n",
    "        * **Batching:** Grouping samples into mini-batches for efficient processing by GPUs.\n",
    "        * **Shuffling:** Randomizing data order for better generalization.\n",
    "        * **`num_workers`:** (Optional) Parallel data loading using multiple CPU processes to prevent GPU idle time.\n",
    "* **Common Pitfalls/Debugging:**\n",
    "    * **`__len__` or `__getitem__` errors:** Incorrect implementation leads to indexing issues.\n",
    "    * **`num_workers > 0` on Windows:** Can sometimes cause issues (related to multiprocessing), `num_workers=0` (main process) is safer on Windows if problems arise.\n",
    "    * **Data not on device:** Forgetting to `inputs.to(device)` and `targets.to(device)` inside the training loop.\n",
    "\n",
    "### 6. The Complete Training Loop\n",
    "\n",
    "* **Concise Definition:** The iterative cycle where a model learns from data by making predictions, measuring error, computing adjustments, and updating its parameters over many epochs and batches.\n",
    "* **\"Why it Matters\" / Importance:** This is the heart of deep learning; it's how models are actually \"trained\" to perform their task.\n",
    "* **Key Steps (The 5-Step Cycle per Batch):**\n",
    "    1.  **Forward Pass:** `predictions = model(inputs)` (Model makes a guess).\n",
    "    2.  **Calculate Loss:** `loss = loss_fn(predictions, targets)` (Measures \"wrongness\").\n",
    "    3.  **Zero Gradients:** `optimizer.zero_grad()` (Clears previous adjustment instructions).\n",
    "    4.  **Backward Pass:** `loss.backward()` (Computes *new* adjustments/gradients).\n",
    "    5.  **Optimizer Step:** `optimizer.step()` (Applies the adjustments to weights).\n",
    "* **Monitoring & Evaluation:**\n",
    "    * `model.train()`: Sets model to training mode (e.g., enables dropout). Call at start of epoch.\n",
    "    * `model.eval()`: Sets model to evaluation/inference mode (e.g., disables dropout). Call before evaluation.\n",
    "    * `with torch.no_grad():`: Disables gradient tracking during evaluation/inference to save memory and speed.\n",
    "    * **Accuracy Calculation:** `torch.round(torch.sigmoid(predictions)) == targets` then `.sum().item()` and divide by `total_samples`.\n",
    "* **Common Pitfalls/Debugging:**\n",
    "    * Forgetting any of the 5 core steps.\n",
    "    * Not switching `model.train()` / `model.eval()`.\n",
    "    * Not using `with torch.no_grad()` for evaluation.\n",
    "    * `Target size` / `Input size` mismatches (often due to `num_classes` or `unsqueeze` issues).\n",
    "    * Overfitting (low training loss/high training accuracy, but poor test performance).\n",
    "\n",
    "### 7. Text Preprocessing (Mini-Project Specific)\n",
    "\n",
    "* **Concise Definition:** Converting raw human language into a numerical, fixed-length format that a neural network can process.\n",
    "* **Key Steps:**\n",
    "    * **Tokenization:** Breaking text into words (e.g., `sentence.lower().replace().split()`).\n",
    "    * **Vocabulary Creation:** Building a dictionary of unique words and assigning them integer IDs, including `<UNK>` (for unknown words) and `<PAD>` (for padding).\n",
    "    * **Numericalization:** Replacing words with their corresponding integer IDs.\n",
    "    * **Padding/Truncation:** Making all sequences the same length by adding `PAD` tokens or cutting longer sequences.\n",
    "* **\"Why not code from scratch in real world?\":**\n",
    "    * While we built it from scratch for *conceptual understanding*, in production, you use highly optimized libraries (e.g., Hugging Face `tokenizers`, spaCy, NLTK).\n",
    "    * These libraries handle complex edge cases, are faster (often in C++/Rust), and are battle-tested.\n",
    "    * **Your Value:** Understanding the *concepts* from scratch allows you to effectively *use, debug, and customize* these powerful libraries.\n",
    "\n",
    "### 8. Debugging Overfitting (from Mini-Project Experience)\n",
    "\n",
    "* **What it is:** When a model performs very well on the training data (memorization) but poorly on new, unseen data (fails to generalize).\n",
    "* **Symptoms:** Low training loss + high training accuracy, but bad performance on a separate validation/test set. (As seen in your mini-project with 100% accuracy on 6 sentences, but wrong predictions on new ones).\n",
    "* **Common Causes (as seen in mini-project):**\n",
    "    * **Extremely Small Dataset:** Not enough diverse examples to learn general rules.\n",
    "    * **Too Many Epochs on Small Data:** Reinforces memorization.\n",
    "    * **Overly Complex Model for Data Size:** Even simple models can overfit tiny datasets.\n",
    "* **Solutions (Conceptually):**\n",
    "    * **More Data (Best Solution):** Gather more diverse training examples.\n",
    "    * **Regularization:** Techniques like Dropout (randomly turning off neurons) or L1/L2 regularization (penalizing large weights).\n",
    "    * **Early Stopping:** Stop training when performance on a *separate validation set* starts to degrade, even if training loss still goes down.\n",
    "    * **Cross-Validation:** For very small datasets, using techniques like k-fold cross-validation can give more robust evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208a571-8999-42c1-9461-0e5704bde469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
