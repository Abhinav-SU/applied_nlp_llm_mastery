{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774b17c0-2a35-40d7-b893-35945ec34abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Reviews:\n",
      "['i', 'love', 'this', 'movie']\n",
      "['this', 'film', 'is', 'great']\n",
      "['the', 'acting', 'was', 'terrible']\n",
      "['i', 'hate', 'this', 'film']\n",
      "['great', 'movie']\n",
      "['it', 'was', 'good']\n",
      "\n",
      "Vocabulary:\n",
      "['<PAD>', '<UNK>', 'acting', 'film', 'good', 'great', 'hate', 'i', 'is', 'it', 'love', 'movie', 'terrible', 'the', 'this', 'was']\n",
      "Vocabulary size: 16\n",
      "\n",
      "Word to Index Mapping (first few):\n",
      "'<PAD>': 0\n",
      "'<UNK>': 1\n",
      "'acting': 2\n",
      "'film': 3\n",
      "'good': 4\n",
      "\n",
      "Index to Word Mapping (first few):\n",
      "0: '<PAD>'\n",
      "1: '<UNK>'\n",
      "2: 'acting'\n",
      "3: 'film'\n",
      "4: 'good'\n"
     ]
    }
   ],
   "source": [
    "# Our tiny dataset (imagine these are movie reviews)\n",
    "raw_reviews = [\n",
    "    \"I love this movie!\",\n",
    "    \"This film is great.\",\n",
    "    \"The acting was terrible.\",\n",
    "    \"I hate this film.\",\n",
    "    \"Great movie!\",\n",
    "    \"It was good.\",\n",
    "]\n",
    "\n",
    "# Step 1: Simple Tokenization (just splitting by space and removing basic punctuation)\n",
    "tokenized_reviews = []\n",
    "for review in raw_reviews:\n",
    "    # Convert to lowercase and remove common punctuation for simplicity\n",
    "    clean_review = review.lower().replace(\"!\", \"\").replace(\".\", \"\").replace(\",\", \"\")\n",
    "    tokens = clean_review.split()\n",
    "    tokenized_reviews.append(tokens)\n",
    "\n",
    "print(\"Tokenized Reviews:\")\n",
    "for tokens in tokenized_reviews:\n",
    "    print(tokens)\n",
    "\n",
    "# Step 2: Vocabulary Creation\n",
    "# We'll use a set to easily get unique words\n",
    "vocabulary_set = set()\n",
    "for tokens in tokenized_reviews:\n",
    "    for token in tokens:\n",
    "        vocabulary_set.add(token)\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = [\"<UNK>\", \"<PAD>\"]\n",
    "for token in special_tokens:\n",
    "    vocabulary_set.add(token)\n",
    "\n",
    "# Convert the set to a sorted list to ensure consistent ordering (important for assigning indices)\n",
    "vocabulary = sorted(list(vocabulary_set))\n",
    "\n",
    "# Create a mapping from word to index (word_to_idx)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Create a mapping from index to word (idx_to_word) for reverse lookup\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "print(\"\\nVocabulary:\")\n",
    "print(vocabulary)\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "print(\"\\nWord to Index Mapping (first few):\")\n",
    "# Print first 5 items to keep it concise\n",
    "for word, idx in list(word_to_idx.items())[:5]:\n",
    "    print(f\"'{word}': {idx}\")\n",
    "\n",
    "print(\"\\nIndex to Word Mapping (first few):\")\n",
    "# Print first 5 items\n",
    "for idx, word in list(idx_to_word.items())[:5]:\n",
    "    print(f\"{idx}: '{word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3710f50-f9ed-4696-b447-83a9e4eb66ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numericalized and Padded Reviews:\n",
      "[7, 10, 14, 11, 0]\n",
      "[14, 3, 8, 5, 0]\n",
      "[13, 2, 15, 12, 0]\n",
      "[7, 6, 14, 3, 0]\n",
      "[5, 11, 0, 0, 0]\n",
      "[9, 15, 4, 0, 0]\n",
      "\n",
      "Numericalized Reviews as PyTorch Tensor:\n",
      "tensor([[ 7, 10, 14, 11,  0],\n",
      "        [14,  3,  8,  5,  0],\n",
      "        [13,  2, 15, 12,  0],\n",
      "        [ 7,  6, 14,  3,  0],\n",
      "        [ 5, 11,  0,  0,  0],\n",
      "        [ 9, 15,  4,  0,  0]])\n",
      "Tensor shape: torch.Size([6, 5])\n",
      "\n",
      "Dummy Labels Tensor:\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Labels shape: torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Assuming 'tokenized_reviews', 'word_to_idx', and 'idx_to_word' from your previous code\n",
    "\n",
    "# Define a maximum sequence length (choose something reasonable for your data)\n",
    "# For our very short sentences, let's pick 5.\n",
    "max_sequence_length = 5\n",
    "pad_idx = word_to_idx[\"<PAD>\"] # Get the index for our padding token\n",
    "\n",
    "numericalized_reviews = []\n",
    "for tokens in tokenized_reviews:\n",
    "    # 1. Numericalize: Convert words to indices\n",
    "    # Use word_to_idx.get(token, word_to_idx[\"<UNK>\"]) to handle potential unknown words\n",
    "    numericalized_tokens = [word_to_idx.get(token, word_to_idx[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "    # 2. Pad or Truncate: Make all sequences max_sequence_length\n",
    "    if len(numericalized_tokens) < max_sequence_length:\n",
    "        # Pad: Add pad_idx until it reaches max_sequence_length\n",
    "        padded_tokens = numericalized_tokens + [pad_idx] * (max_sequence_length - len(numericalized_tokens))\n",
    "    else:\n",
    "        # Truncate: If a review is longer than max_sequence_length, cut it short\n",
    "        padded_tokens = numericalized_tokens[:max_sequence_length]\n",
    "\n",
    "    numericalized_reviews.append(padded_tokens)\n",
    "\n",
    "print(\"\\nNumericalized and Padded Reviews:\")\n",
    "for review_indices in numericalized_reviews:\n",
    "    print(review_indices)\n",
    "\n",
    "# You can convert this to a PyTorch tensor now\n",
    "numericalized_reviews_tensor = torch.tensor(numericalized_reviews, dtype=torch.long)\n",
    "print(\"\\nNumericalized Reviews as PyTorch Tensor:\")\n",
    "print(numericalized_reviews_tensor)\n",
    "print(\"Tensor shape:\", numericalized_reviews_tensor.shape)\n",
    "\n",
    "# Let's also define some dummy labels for our reviews\n",
    "# 1 for positive, 0 for negative\n",
    "# Matches raw_reviews: \"I love this movie!\", \"This film is great.\", \"The acting was terrible.\", \"I hate this film.\", \"Great movie!\", \"It was good.\",\n",
    "dummy_labels = torch.tensor([1, 1, 0, 0, 1, 1], dtype=torch.float).unsqueeze(1) # unsqueeze(1) makes it a column vector\n",
    "print(\"\\nDummy Labels Tensor:\")\n",
    "print(dummy_labels)\n",
    "print(\"Labels shape:\", dummy_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a76fe26-3b64-4d6f-bc41-b3f7fb75b653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in sentiment dataset: 6\n",
      "\n",
      "Iterating through DataLoader with batch_size=2:\n",
      "Batch 1: Text batch shape torch.Size([2, 5]), Label batch shape torch.Size([2, 1])\n",
      "Text batch content:\n",
      "tensor([[14,  3,  8,  5,  0],\n",
      "        [ 7, 10, 14, 11,  0]])\n",
      "Label batch content:\n",
      "tensor([[1.],\n",
      "        [1.]])\n",
      "\n",
      "Batch 2: Text batch shape torch.Size([2, 5]), Label batch shape torch.Size([2, 1])\n",
      "Text batch content:\n",
      "tensor([[ 9, 15,  4,  0,  0],\n",
      "        [ 7,  6, 14,  3,  0]])\n",
      "Label batch content:\n",
      "tensor([[1.],\n",
      "        [0.]])\n",
      "\n",
      "Batch 3: Text batch shape torch.Size([2, 5]), Label batch shape torch.Size([2, 1])\n",
      "Text batch content:\n",
      "tensor([[13,  2, 15, 12,  0],\n",
      "        [ 5, 11,  0,  0,  0]])\n",
      "Label batch content:\n",
      "tensor([[0.],\n",
      "        [1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Assuming numericalized_reviews_tensor and dummy_labels are available from the previous cell's output\n",
    "\n",
    "class TextSentimentDataset(Dataset):\n",
    "    def __init__(self, data_tensor, labels_tensor):\n",
    "        # Store the preprocessed data and labels\n",
    "        self.data = data_tensor\n",
    "        self.labels = labels_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a single data sample (numericalized text and its label)\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Instantiate our custom dataset\n",
    "sentiment_dataset = TextSentimentDataset(numericalized_reviews_tensor, dummy_labels)\n",
    "print(f\"Total samples in sentiment dataset: {len(sentiment_dataset)}\")\n",
    "\n",
    "# Create a DataLoader\n",
    "# We'll use a small batch size for our tiny dataset\n",
    "batch_size = 2 # Let's process 2 reviews at a time\n",
    "shuffle = True # Important for training to randomize batches\n",
    "\n",
    "sentiment_dataloader = DataLoader(sentiment_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "print(f\"\\nIterating through DataLoader with batch_size={batch_size}:\")\n",
    "for i, (text_batch, label_batch) in enumerate(sentiment_dataloader):\n",
    "    print(f\"Batch {i+1}: Text batch shape {text_batch.shape}, Label batch shape {label_batch.shape}\")\n",
    "    print(f\"Text batch content:\\n{text_batch}\")\n",
    "    print(f\"Label batch content:\\n{label_batch}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de0afda1-de68-4c42-b941-1a12ff020786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Starting Training for Sentiment Classifier...\n",
      "Epoch [2/100], Loss: 0.5832, Accuracy: 1.0000\n",
      "Epoch [10/100], Loss: 0.1798, Accuracy: 1.0000\n",
      "Epoch [20/100], Loss: 0.0313, Accuracy: 1.0000\n",
      "Epoch [30/100], Loss: 0.0095, Accuracy: 1.0000\n",
      "Epoch [40/100], Loss: 0.0048, Accuracy: 1.0000\n",
      "Epoch [50/100], Loss: 0.0030, Accuracy: 1.0000\n",
      "Epoch [60/100], Loss: 0.0021, Accuracy: 1.0000\n",
      "Epoch [70/100], Loss: 0.0016, Accuracy: 1.0000\n",
      "Epoch [80/100], Loss: 0.0012, Accuracy: 1.0000\n",
      "Epoch [90/100], Loss: 0.0010, Accuracy: 1.0000\n",
      "Epoch [100/100], Loss: 0.0008, Accuracy: 1.0000\n",
      "\n",
      "Training Finished!\n",
      "\n",
      "--- Testing Inference ---\n",
      "'This is a great film!' -> ('Positive', 0.9802723526954651)\n",
      "'This movie was awful.' -> ('Positive', 0.9640846848487854)\n",
      "'It was ok.' -> ('Positive', 0.9994801878929138)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Re-define our Dataset (if you're running this in a new session or fresh notebook) ---\n",
    "# If you ran the previous cells in this session, these variables should already be in memory.\n",
    "raw_reviews = [\n",
    "    \"I love this movie!\", \"This film is great.\", \"The acting was terrible.\",\n",
    "    \"I hate this film.\", \"Great movie!\", \"It was good.\"\n",
    "]\n",
    "tokenized_reviews = []\n",
    "for review in raw_reviews:\n",
    "    clean_review = review.lower().replace(\"!\", \"\").replace(\".\", \"\").replace(\",\", \"\")\n",
    "    tokens = clean_review.split()\n",
    "    tokenized_reviews.append(tokens)\n",
    "\n",
    "vocabulary_set = set()\n",
    "for tokens in tokenized_reviews:\n",
    "    for token in tokens:\n",
    "        vocabulary_set.add(token)\n",
    "special_tokens = [\"<UNK>\", \"<PAD>\"]\n",
    "for token in special_tokens:\n",
    "    vocabulary_set.add(token)\n",
    "vocabulary = sorted(list(vocabulary_set))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "max_sequence_length = 5\n",
    "pad_idx = word_to_idx[\"<PAD>\"]\n",
    "numericalized_reviews = []\n",
    "for tokens in tokenized_reviews:\n",
    "    numericalized_tokens = [word_to_idx.get(token, word_to_idx[\"<UNK>\"]) for token in tokens]\n",
    "    if len(numericalized_tokens) < max_sequence_length:\n",
    "        padded_tokens = numericalized_tokens + [pad_idx] * (max_sequence_length - len(numericalized_tokens))\n",
    "    else:\n",
    "        padded_tokens = numericalized_tokens[:max_sequence_length]\n",
    "    numericalized_reviews.append(padded_tokens)\n",
    "numericalized_reviews_tensor = torch.tensor(numericalized_reviews, dtype=torch.long)\n",
    "dummy_labels = torch.tensor([1, 1, 0, 0, 1, 1], dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "\n",
    "# --- Redefine our TextSentimentDataset ---\n",
    "class TextSentimentDataset(Dataset):\n",
    "    def __init__(self, data_tensor, labels_tensor):\n",
    "        self.data = data_tensor\n",
    "        self.labels = labels_tensor\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# --- Redefine our Classifier Model (adapted from MySimpleRegressor) ---\n",
    "class SimpleSentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
    "        super(SimpleSentimentClassifier, self).__init__()\n",
    "        # NEW: Embedding layer for text data!\n",
    "        # It takes word indices and converts them into dense vectors (embeddings).\n",
    "        # vocab_size: total number of unique words in our vocabulary\n",
    "        # embedding_dim: the size of the vector representation for each word (e.g., 50, 100, 300)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Our input to the first linear layer will be the flattened embedding of the sequence.\n",
    "        # So, input_dim for fc1 is embedding_dim * max_sequence_length\n",
    "        self.fc1 = nn.Linear(embedding_dim * max_sequence_length, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # output_size is 1 for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x comes in as (batch_size, max_sequence_length) with word indices\n",
    "\n",
    "        # 1. Pass through embedding layer\n",
    "        # Output shape: (batch_size, max_sequence_length, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 2. Flatten the embeddings for the feedforward layer\n",
    "        # Output shape: (batch_size, max_sequence_length * embedding_dim)\n",
    "        x = x.view(x.size(0), -1) # x.size(0) is batch_size, -1 infers remaining dimensions\n",
    "\n",
    "        # 3. Pass through linear layers with ReLU activation\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        # For binary classification with BCEWithLogitsLoss, we typically don't apply\n",
    "        # Sigmoid here. The loss function handles it internally for numerical stability.\n",
    "        return x\n",
    "\n",
    "# --- Configuration for our training ---\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 10 # Each word will be represented by a 10-dimensional vector\n",
    "hidden_dim = 20    # Number of neurons in the hidden layer\n",
    "output_dim = 1     # 1 output neuron for binary classification\n",
    "max_seq_len = max_sequence_length # From our preprocessing\n",
    "learning_rate = 0.005\n",
    "epochs = 100 # Increased epochs for small dataset to see convergence\n",
    "batch_size = 2 # Small batch size for our tiny dataset\n",
    "\n",
    "# 1. Instantiate the Dataset and DataLoader\n",
    "sentiment_dataset = TextSentimentDataset(numericalized_reviews_tensor, dummy_labels)\n",
    "sentiment_dataloader = DataLoader(sentiment_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 2. Instantiate the Model\n",
    "model = SimpleSentimentClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# 3. Define Loss Function (Binary Cross-Entropy with Logits for stability)\n",
    "# BCEWithLogitsLoss combines Sigmoid and Binary Cross Entropy,\n",
    "# which is more numerically stable than applying Sigmoid then BCE separately.\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 4. Define Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check if GPU is available and move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- The Training Loop ---\n",
    "print(\"\\nStarting Training for Sentiment Classifier...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train() # Set model to training mode\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(sentiment_dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(predictions, targets) # predictions are logits, targets are 0s/1s\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(sentiment_dataloader)\n",
    "\n",
    "    # Simple evaluation on training data (just for quick check)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # No need to calculate gradients for evaluation\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for inputs, targets in sentiment_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            predictions = model(inputs)\n",
    "            # Apply sigmoid to convert logits to probabilities, then round to 0 or 1\n",
    "            predicted_classes = torch.round(torch.sigmoid(predictions))\n",
    "            total_correct += (predicted_classes == targets).sum().item()\n",
    "            total_samples += targets.numel()\n",
    "        accuracy = total_correct / total_samples\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 1: # Print every 10 epochs and first epoch\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nTraining Finished!\")\n",
    "\n",
    "# --- Test with a new (unknown) sentence ---\n",
    "# This part won't work perfectly as the model is tiny and dataset is small,\n",
    "# but it shows the inference process.\n",
    "print(\"\\n--- Testing Inference ---\")\n",
    "def predict_sentiment(text, model, word_to_idx, max_seq_len, device):\n",
    "    model.eval() # Set to evaluation mode\n",
    "    clean_text = text.lower().replace(\"!\", \"\").replace(\".\", \"\").replace(\",\", \"\")\n",
    "    tokens = clean_text.split()\n",
    "    numericalized_tokens = [word_to_idx.get(token, word_to_idx[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "    if len(numericalized_tokens) < max_seq_len:\n",
    "        padded_tokens = numericalized_tokens + [word_to_idx[\"<PAD>\"]] * (max_seq_len - len(numericalized_tokens))\n",
    "    else:\n",
    "        padded_tokens = numericalized_tokens[:max_seq_len]\n",
    "\n",
    "    input_tensor = torch.tensor(padded_tokens, dtype=torch.long).unsqueeze(0).to(device) # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_logits = model(input_tensor)\n",
    "        prediction_prob = torch.sigmoid(output_logits).item() # Convert logits to probability\n",
    "        predicted_class = \"Positive\" if prediction_prob >= 0.5 else \"Negative\"\n",
    "        return predicted_class, prediction_prob\n",
    "\n",
    "# Example new sentences\n",
    "test_sentence1 = \"This is a great film!\"\n",
    "test_sentence2 = \"This movie was awful.\"\n",
    "test_sentence3 = \"It was ok.\" # Might be tricky for tiny model\n",
    "\n",
    "print(f\"'{test_sentence1}' -> {predict_sentiment(test_sentence1, model, word_to_idx, max_sequence_length, device)}\")\n",
    "print(f\"'{test_sentence2}' -> {predict_sentiment(test_sentence2, model, word_to_idx, max_sequence_length, device)}\")\n",
    "print(f\"'{test_sentence3}' -> {predict_sentiment(test_sentence3, model, word_to_idx, max_sequence_length, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e02b6824-f20c-4d7e-8ca0-df43095aa5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'love', 'this', 'movie'], ['this', 'film', 'is', 'great'], ['the', 'acting', 'was', 'terrible'], ['i', 'hate', 'this', 'film'], ['great', 'movie'], ['it', 'was', 'good']]\n",
      "\n",
      "Numericalized Reviews as PyTorch Tensor:\n",
      "tensor([[ 7, 10, 14, 11,  0],\n",
      "        [14,  3,  8,  5,  0],\n",
      "        [13,  2, 15, 12,  0],\n",
      "        [ 7,  6, 14,  3,  0],\n",
      "        [ 5, 11,  0,  0,  0],\n",
      "        [ 9, 15,  4,  0,  0]])\n",
      "Tensor shape: torch.Size([6, 5])\n",
      "\n",
      "Labels Tensor:\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Labels shape: torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "movie_reviews=[\"I love this movie!\",\"This film is great.\",\"The acting was terrible.\",\"I hate this film.\",\"Great movie!\",\"It was good.\"]\n",
    "moview_labels=[1,1,0,0,1,1]\n",
    "\n",
    "tokenized_reviews= [review.lower().replace(\"!\",\"\").replace(\".\",\"\").split() for review in movie_reviews]\n",
    "print(tokenized_reviews)\n",
    "\n",
    "all_words = [word for words in tokenized_reviews for word in words]\n",
    "unique_words = set(all_words)\n",
    "\n",
    "unique_words.add(\"<UNK>\")\n",
    "unique_words.add(\"<PAD>\")\n",
    "\n",
    "vocabulary=[]\n",
    "vocabulary=sorted(unique_words)\n",
    "\n",
    "word_to_idx={word : indx for indx,word in enumerate(vocabulary)}\n",
    "idx_to_word={indx:word for indx, word in enumerate(vocabulary)}\n",
    "\n",
    "max_sequence_length=5\n",
    "pad_idx = word_to_idx[\"<PAD>\"]\n",
    "numericalized_padded_reviews_list=[]\n",
    "\n",
    "\n",
    "\n",
    "for tokens in tokenized_reviews:\n",
    "    \n",
    "    current_numerical_sequence=[word_to_idx.get(word,word_to_idx[\"<UNK>\"]) for word in tokens]\n",
    "    \n",
    "    if len(current_numerical_sequence) < max_sequence_length:\n",
    "        \n",
    "        padding_needed = max_sequence_length - len(current_numerical_sequence)\n",
    "        final_sequence_for_nn = current_numerical_sequence + [pad_idx] * padding_needed\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        final_sequence_for_nn = current_numerical_sequence[:max_sequence_length]\n",
    "    \n",
    "    numericalized_padded_reviews_list.append(final_sequence_for_nn)\n",
    "\n",
    "    \n",
    "numericalized_reviews_tensor = torch.tensor(numericalized_padded_reviews_list, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(moview_labels, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "print(\"\\nNumericalized Reviews as PyTorch Tensor:\")\n",
    "print(numericalized_reviews_tensor)\n",
    "print(\"Tensor shape:\", numericalized_reviews_tensor.shape)\n",
    "\n",
    "print(\"\\nLabels Tensor:\")\n",
    "print(labels_tensor)\n",
    "print(\"Labels shape:\", labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa3048e-612c-4ad4-860b-d809e88cce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class TextSentimentDataset(Dataset):\n",
    "    def __init__(self,data_tensor,label_tensor):\n",
    "        super().__init__()\n",
    "        self.data = data_tensor\n",
    "        self.labels = label_tensor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self,x):\n",
    "        return self.data[x],self.labels[x]\n",
    "\n",
    "\n",
    "sentiment_dataset = TextSentimentDataset(numericalized_reviews_tensor,labels_tensor)\n",
    "\n",
    "sentiment_dataloader = DataLoader(sentiment_dataset,batch_size=32,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809def6-4fb3-49af-8c9a-02f0efee5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleSentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size,max_sequence_length , embedding_dim, hidden_size, num_classes=2):\n",
    "        super().__init__()\n",
    "        # We need to define our layers here\n",
    "        self.embedding_layer = nn.Embedding(vocab_size,embedding_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(max_sequence_length *embedding_dim,hidden_size)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size,num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.embedding_layer(x)\n",
    "        x=x.view(x.size(0), -1)\n",
    "        x=self.fc1(x)\n",
    "        x=F.relu(x)\n",
    "        return self.fc2(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
